{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1\n",
    "We need to get the data from the file assets/companies_small_set.data into a DataFrame. The problem is that the data on each line of the file is in either a JSON or Tab-separated values (TSV) format.\n",
    "\n",
    "The JSON lines are in the correct format, they just need to be converted to native Python dicts.\n",
    "\n",
    "The TSV lines need to be converted in to dicts that match the JSON format.\n",
    "\n",
    "Write a generator gen_fixed_data that takes an iterator as an arguement. It should parse the values in the iterator and yield each value in the correct format: A dict with the keys:\n",
    "\n",
    "company\n",
    "catch_phrase\n",
    "phone\n",
    "timezone\n",
    "client_count\n",
    "\n",
    "Note that your solution should be a generator function, it should not return a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def gen_fix_data(data_iterator):\n",
    "    for line in data_iterator:\n",
    "        line = line.strip()\n",
    "        if line.startswith('{'):\n",
    "            yield json.loads(line)\n",
    "        else: \n",
    "            fields = line.split('\\t')\n",
    "            yield {\n",
    "                \"company\": fields[0],\n",
    "                \"catch_phrase\": fields[1],\n",
    "                \"phone\": fields[2],\n",
    "                \"timezone\": fields[3],\n",
    "                \"client_count\": int(fields[4])\n",
    "            }\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "1. Import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json: This module helps parse JSON formatted strings into Python dictionaries.\n",
    "\n",
    "pandas as pd: This library is useful for data manipulation and analysis, but in this function, itâ€™s mostly imported for later use when converting the parsed data to a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the generator function gen_fix_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fix_data(data_iterator):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_iterator: This parameter is an iterator that yields lines of data, each line being either JSON or TSV formatted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Iterate over each line in the data iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in data_iterator:\n",
    "    line = line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "line.strip(): Removes any leading and trailing whitespace, including newlines, from the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Check if the line is in JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if line.startswith('{'):\n",
    "    yield json.loads(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "line.startswith('{'): Determines if the line is JSON by checking if it starts with a {.\n",
    "\n",
    "json.loads(line): Converts the JSON formatted string into a Python dictionary and yields it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Handle TSV formatted lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "else:\n",
    "    fields = line.split('\\t')\n",
    "    yield {\n",
    "        \"company\": fields[0],\n",
    "        \"catch_phrase\": fields[1],\n",
    "        \"phone\": fields[2],\n",
    "        \"timezone\": fields[3],\n",
    "        \"client_count\": int(fields[4])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "line.split('\\t'): Splits the TSV line into its respective fields using the tab character as the delimiter.\n",
    "\n",
    "Constructing the dictionary: Creates a dictionary with the required keys (company, catch_phrase, phone, timezone, and client_count). The values are extracted from the split fields, and client_count is explicitly converted to an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So, the gen_fix_data generator function processes each line of data, determining its format, converting it to a consistent dictionary format, and yielding the resulting dictionaries. These can then be used to create a DataFrame or for other analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2\n",
    "The data in assets/server_metrics.csv represents the time it take to handle requests in a start-up company's web application. Let's imagine we are asked to write some code that gives us a DataFrame that just contains the entries where processing_time is greater than 160 milliseconds.\n",
    "\n",
    "We could solve that problem like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('assets/server_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = df[df['processing_time'] > 160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = outliers['processing_time'].plot.hist(title=\"Times > 160\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But imagine that instead of dealing with millions of rows, we have to deal with billions or trillions and the set is too big to fit comfortably in memory, or that the data is coming to us not in a local file, but is being read over the network. Generators can be a nice way to help in that situation.\n",
    "\n",
    "Here is a generator that yields a dict for each line in assets/server_metrics.csv.\n",
    "\n",
    "Note that your solution should be a generator function, it should not return a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_stream():\n",
    "    '''\n",
    "    Generate dictionaries from each line in assets/server_metrics.csv\n",
    "    '''\n",
    "    import csv\n",
    "\n",
    "    with open('assets/server_metrics.csv', 'r') as stream:\n",
    "        csv_stream = csv.DictReader(stream, ['job_id', 'processing_time', 'instance_id'])\n",
    "        next(csv_stream) # throw away header row\n",
    "        for entry in csv_stream:\n",
    "            entry['processing_time'] = float(entry['processing_time'])\n",
    "            yield dict(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, write a generator that can be used to create a DataFrame like the outliers one above. Its first parameter should be the iterable we get from the metrics_stream() generator function. Its second (optional) parameter should be called lower_bound and be used to filter out entries whose \"processing_time\" is less than or equal to this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_outliers(metrics_iterable, lower_bound=160):\n",
    "    for metric in metrics_iterable:\n",
    "        if metric['processing_time'] > lower_bound:\n",
    "            yield metric\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Function Definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_outliers(metrics_iterable, lower_bound=160):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines the generator function gen_outliers that takes two parameters: metrics_iterable (an iterable of metrics) and lower_bound (an optional parameter with a default value of 160)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Iteration and Filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics_iterable:\n",
    "    if metric['processing_time'] > lower_bound:\n",
    "        yield metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop iterates through each item in metrics_iterable. For each metric, it checks if the processing_time is greater than lower_bound. If it is, it yields (returns) the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generating Outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_gen = metrics_stream()\n",
    "generated_outliers = pd.DataFrame(gen_outliers(metrics_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the metrics_stream generator creates an iterable of metrics, and gen_outliers processes this iterable to filter out only those metrics where processing_time is greater than 160. These filtered metrics are then converted into a pandas DataFrame named generated_outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plotting the Histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_ = generated_outliers['processing_time'].plot.hist(title=\"Times > 160\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code imports matplotlib.pyplot for plotting and then creates a histogram of the processing_time values in generated_outliers. The histogram is titled \"Times > 160\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In summary, this solution leverages a generator to efficiently filter out metrics with processing_time greater than 160 milliseconds and then creates a histogram to visualize these outliers. This approach is especially useful when dealing with large datasets, as it processes the data in a memory-efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3\n",
    "Write a decorator called as_json that converts the wrapped function's return value to a JSON encoded string.\n",
    "\n",
    "You can assume that this will only be used on functions whose return values can be converted to JSON.\n",
    "This will be easiest if you use the standard library's json package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import wraps\n",
    "\n",
    "def as_json(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        return json.dumps(result)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json is imported for encoding data as JSON strings.\n",
    "\n",
    "wraps is imported to ensure the wrapper function retains the metadata of the original function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the Decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_json(func):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the definition of the as_json decorator. It takes a function func as its argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define the Wrapper Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wraps(func)\n",
    "def wrapper(*args, **kwargs):\n",
    "    result = func(*args, **kwargs)\n",
    "    return json.dumps(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@wraps(func): This decorator ensures that the wrapper function retains the original functionâ€™s name, docstring, and other metadata.\n",
    "\n",
    "def wrapper(*args, **kwargs): This defines the wrapper function which takes any number of positional and keyword arguments.\n",
    "\n",
    "result = func(*args, **kwargs): Calls the original function func with the given arguments and stores the result.\n",
    "\n",
    "return json.dumps(result): Converts the result to a JSON encoded string and returns it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Return the Wrapper Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, you can use this decorator to convert the return value of any function to a JSON encoded string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "@as_json\n",
    "def get_data():\n",
    "    return {'name': 'Alice', 'age': 30}\n",
    "\n",
    "print(get_data()) # Output: {\"name\": \"Alice\", \"age\": 30}\n",
    "\n",
    "# This example demonstrates how the as_json decorator converts the dictionary \n",
    "# returned by get_data into a JSON string. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
