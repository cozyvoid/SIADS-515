{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"REPLACE_PACKAGE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIADS 515 Week 2 Homework (HW2)\n",
    "A Pandas DataFrame can be populated with a generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_three_data():\n",
    "    '''\n",
    "    Generate 3 dictionaries with keys A and B\n",
    "    '''\n",
    "    for i in range(3):\n",
    "        yield {\n",
    "            \"A\": i + 1, # simple\n",
    "            \"B\": (i + 1) * 10, # some math\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(data=gen_three_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add a parameter to the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def gen_some_data(n):\n",
    "    '''\n",
    "    Generate n dictionaries with keys A, B and C\n",
    "    '''\n",
    "    for i in range(n):\n",
    "        yield {\n",
    "            \"A\": i + 1, # simple\n",
    "            \"B\": (i + 1) * 10, # some math\n",
    "            \"C\": np.random.randn() # a random number drawn from a standard normal distribution\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(data=gen_some_data(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are times when using this technique can be a nice way to solve problems that would be hard to solve in other ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1\n",
    "We need to get the data from the file assets/companies_small_set.data into a DataFrame. The problem is that the data on each line of the file is in either a JSON or Tab-separated values (TSV) format.\n",
    "\n",
    "The JSON lines are in the correct format, they just need to be converted to native Python dicts.\n",
    "\n",
    "The TSV lines need to be converted in to dicts that match the JSON format.\n",
    "\n",
    "Write a generator gen_fixed_data that takes an iterator as an arguement. It should parse the values in the iterator and yield each value in the correct format: A dict with the keys:\n",
    "\n",
    "company\n",
    "catch_phrase\n",
    "phone\n",
    "timezone\n",
    "client_count\n",
    "Note that your solution should be a generator function, it should not return a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def gen_fix_data(data_iterator):\n",
    "    # YOUR CODE HERE\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the generator is correctly fixing the data formats, we should be able to use it to populate a DataFrame..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('assets/companies_small_set.data', 'r') as broken_data:\n",
    "    df = pd.DataFrame(data=gen_fix_data(broken_data))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell runs a series of assert statements to grade your solution.\n",
    "\n",
    "with open('assets/companies_small_set.data', 'r') as broken_data:\n",
    "    gen = gen_fix_data(broken_data)\n",
    "    \n",
    "    # Let's make sure gen_fix_data is a generator function...\n",
    "    from types import GeneratorType\n",
    "    assert type(gen) == GeneratorType, 'wrong type, should be a generator'\n",
    "    \n",
    "\n",
    "    # Check the first entry from companies_small_set.data...\n",
    "    entry1 = next(gen)\n",
    "    assert entry1['company'] == \"Watkins Inc\", 'incorrect value for entry1[\"company\"]'\n",
    "    assert entry1['catch_phrase'] == \"Integrated radical installation\", \\\n",
    "        'incorrect value for entry1[\"catch_phrase\"]'\n",
    "    assert entry1['phone'] == '7712422719', 'incorrect value for entry1[\"phone\"]'\n",
    "    assert entry1['timezone'] == \"America/New_York\", 'incorrect value for entry1[\"timezone\"]'\n",
    "    assert type(entry1['client_count']) == int, 'entry1[\"client_count\"] is not an int'\n",
    "    assert entry1['client_count'] == 442, 'incorrect value for entry1[\"client_count\"]'\n",
    "\n",
    "    # Check the second entry from companies_small_set.data...\n",
    "    entry2 = next(gen)\n",
    "    assert entry2['company'] == \"Bennett and Sons\", 'incorrect value for entry2[\"company\"]'\n",
    "    assert entry2['catch_phrase'] == \"Persistent contextually-based standardization\", \\\n",
    "        'incorrect value for entry2[\"catch_phrase\"]'\n",
    "    assert entry2['phone'] == \"018.666.0600\", 'incorrect value for entry2[\"phone\"]'\n",
    "    assert entry2['timezone'] == \"America/Los_Angeles\", 'incorrect value for entry2[\"timezone\"]'\n",
    "    assert type(entry2['client_count']) == int, 'entry2[\"client_count\"] is not an int'\n",
    "    assert entry2['client_count'] == 492, 'incorrect value for entry2[\"client_count\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell runs a series of assert statements to grade your solution against different data.\n",
    "import io\n",
    "\n",
    "test_data = io.StringIO(\n",
    "    'Chang, Fisher and Green\tOpen-architected foreground productivity\t759.382.4219\t'\n",
    "        'America/Los_Angeles\t770\\n'\n",
    "    'Patel, Thornton and Guzman\tCustomizable asynchronous approach\t+1-578-156-5938x77840\t'\n",
    "        'America/Los_Angeles\t418\\n'\n",
    "    'Smith-Cortez\tIntegrated solution-oriented moratorium\t7535139332\t'\n",
    "        'America/Los_Angeles\t634\\n'\n",
    "    '{\"company\": \"Miller-Flores\", \"catch_phrase\": \"Object-based user-facing array\", \"phone\": \"(185)839-8947x19659\", '\n",
    "        '\"timezone\": \"America/New_York\", \"client_count\": 634}\\n'\n",
    ")\n",
    "generated = pd.DataFrame(gen_fix_data(test_data))\n",
    "\n",
    "correct = pd.DataFrame([{'company': 'Chang, Fisher and Green',\n",
    "  'catch_phrase': 'Open-architected foreground productivity',\n",
    "  'phone': '759.382.4219',\n",
    "  'timezone': 'America/Los_Angeles',\n",
    "  'client_count': 770},\n",
    " {'company': 'Patel, Thornton and Guzman',\n",
    "  'catch_phrase': 'Customizable asynchronous approach',\n",
    "  'phone': '+1-578-156-5938x77840',\n",
    "  'timezone': 'America/Los_Angeles',\n",
    "  'client_count': 418},\n",
    " {'company': 'Smith-Cortez',\n",
    "  'catch_phrase': 'Integrated solution-oriented moratorium',\n",
    "  'phone': '7535139332',\n",
    "  'timezone': 'America/Los_Angeles',\n",
    "  'client_count': 634},\n",
    " {'company': 'Miller-Flores',\n",
    "  'catch_phrase': 'Object-based user-facing array',\n",
    "  'phone': '(185)839-8947x19659',\n",
    "  'timezone': 'America/New_York',\n",
    "  'client_count': 634},\n",
    "])\n",
    "\n",
    "assert len(generated) == len(correct), 'wrong number of rows'\n",
    "assert all(g == c for g, c in zip(generated.columns.sort_values(), correct.columns.sort_values())), \\\n",
    "    'columns names do not match'\n",
    "\n",
    "for col in generated.columns:\n",
    "    for i in range(len(generated)):\n",
    "        assert generated[col][i] == correct[col][i], \\\n",
    "            f'wrong value at column \"{col}\", index \"{i}\", {generated[col][i]} != {correct[col][i]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2\n",
    "The data in assets/server_metrics.csv represents the time it take to handle requests in a start-up company's web application. Let's imagine we are asked to write some code that gives us a DataFrame that just contains the entries where processing_time is greater than 160 milliseconds.\n",
    "\n",
    "We could solve that problem like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('assets/server_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = df[df['processing_time'] > 160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = outliers['processing_time'].plot.hist(title=\"Times > 160\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But imagine that instead of dealing with millions of rows, we have to deal with billions or trillions and the set is too big to fit comfortably in memory, or that the data is coming to us not in a local file, but is being read over the network. Generators can be a nice way to help in that situation.\n",
    "\n",
    "Here is a generator that yields a dict for each line in assets/server_metrics.csv.\n",
    "\n",
    "Note that your solution should be a generator function, it should not return a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_stream():\n",
    "    '''\n",
    "    Generate dictionaries from each line in assets/server_metrics.csv\n",
    "    '''\n",
    "    import csv\n",
    "\n",
    "    with open('assets/server_metrics.csv', 'r') as stream:\n",
    "        csv_stream = csv.DictReader(stream, ['job_id', 'processing_time', 'instance_id'])\n",
    "        next(csv_stream) # throw away header row\n",
    "        for entry in csv_stream:\n",
    "            entry['processing_time'] = float(entry['processing_time'])\n",
    "            yield dict(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, write a generator that can be used to create a DataFrame like the outliers one above. Its first parameter should be the iterable we get from the metrics_stream() generator function. Its second (optional) parameter should be called lower_bound and be used to filter out entries whose \"processing_time\" is less than or equal to this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_outliers(metrics_iterable, lower_bound=160):\n",
    "    # YOUR CODE HERE\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_gen = metrics_stream() \n",
    "\n",
    "generated_outliers = pd.DataFrame(gen_outliers(metrics_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should generate the same plot as the plot above\n",
    "_ = generated_outliers['processing_time'].plot.hist(title=\"Times > 160\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell runs a series of assert statements to grade your solution.\n",
    "\n",
    "gen = gen_outliers(metrics_stream())\n",
    "    \n",
    "# Let's make sure gen_fix_data is a generator function...\n",
    "from types import GeneratorType\n",
    "assert type(gen) == GeneratorType, 'wrong type, should be a generator'\n",
    "\n",
    "# check that data matches\n",
    "\n",
    "outliers_160 = pd.DataFrame(gen_outliers(metrics_stream(), lower_bound=160))\n",
    "assert len(outliers_160) == 2615, 'wrong number of entries for lower_bound=160'\n",
    "\n",
    "outliers_150 = pd.DataFrame(gen_outliers(metrics_stream(), lower_bound=150))\n",
    "assert len(outliers_150) == 556826, 'wrong number of entries for lower_bound=150'\n",
    "\n",
    "outliers_170 = pd.DataFrame(gen_outliers(metrics_stream(), lower_bound=170))\n",
    "assert len(outliers_170) == 9, 'wrong number of entries for lower_bound=170'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell runs a series of assert statements to grade your solution against different data.\n",
    "import io\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        'job_id': '336',\n",
    "        'processing_time': 150.83086863345971,\n",
    "        'instance_id': '1346846',\n",
    "    },\n",
    "    {\n",
    "        'job_id': '337',\n",
    "        'processing_time': 168.37830864466645,\n",
    "        'instance_id': '1349783',\n",
    "    },\n",
    "    {\n",
    "         'job_id': '338',\n",
    "         'processing_time': 148.8572313268281,\n",
    "         'instance_id': '1345472',\n",
    "    },\n",
    "    {\n",
    "        'job_id': '339',\n",
    "        'processing_time': 148.39006806562258,\n",
    "        'instance_id': '1347784',\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "outliers_160 = pd.DataFrame(gen_outliers(test_data, lower_bound=160))\n",
    "assert len(outliers_160) == 1, 'wrong number of entries for lower_bound=160'\n",
    "\n",
    "outliers_150 = pd.DataFrame(gen_outliers(test_data, lower_bound=150))\n",
    "assert len(outliers_150) == 2, 'wrong number of entries for lower_bound=150'\n",
    "\n",
    "outliers_170 = pd.DataFrame(gen_outliers(test_data, lower_bound=170))\n",
    "assert len(outliers_170) == 0, 'wrong number of entries for lower_bound=170'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3\n",
    "Write a decorator called as_json that converts the wrapped function's return value to a JSON encoded string.\n",
    "\n",
    "You can assume that this will only be used on functions whose return values can be converted to JSON.\n",
    "This will be easiest if you use the standard library's json package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@as_json\n",
    "def simple_json_string():\n",
    "    return \"hello\"\n",
    "   \n",
    "assert simple_json_string() == '\"hello\"', \"Did not get the expected output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@as_json\n",
    "def simple_json_list():\n",
    "    return [1, 2, 3, 4]\n",
    "   \n",
    "assert simple_json_list() == \"[1, 2, 3, 4]\", \"Did not get the expected output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@as_json\n",
    "def string_test(): \n",
    "    return \"string\"\n",
    "\n",
    "assert isinstance(string_test(), str), \"Functions wrapped with @as_json should return a str type\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@as_json\n",
    "def func_with_args(name=\"World\"):\n",
    "    return f\"Hello, {name}!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert func_with_args() == '\"Hello, World!\"', \"Did not handle default argument\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert func_with_args(\"Samantha\") == '\"Hello, Samantha!\"', \"Did not handle position argument\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert func_with_args(name=\"Omar\") == '\"Hello, Omar!\"', \"Did not handle key word argument\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
