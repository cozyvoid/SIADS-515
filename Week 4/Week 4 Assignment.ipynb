{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "with open('t8.shakespeare.txt', 'r') as file:\n",
    "    # Initialize a counter\n",
    "    line_count = 0\n",
    "    # Iterate through each line in the file\n",
    "    for line in file:\n",
    "        # Increment the counter for each line\n",
    "        line_count += 1\n",
    "\n",
    "print(f'The file has {line_count} lines.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docdist1.py\n",
    "#\n",
    "#\n",
    "# This program computes the \"distance\" between two text files\n",
    "# as the angle between their word frequency vectors.\n",
    "#\n",
    "# For each input file, a word-frequency vector is computed as follows:\n",
    "#    (1) the specified file is read in\n",
    "#    (2) it is converted into a list of alphanumeric \"words\"\n",
    "#        Here a \"word\" is a sequence of consecutive alphanumeric\n",
    "#        characters.  Non-alphanumeric characters are treated as blanks.\n",
    "#        Case is not significant.\n",
    "#    (3) for each word, its frequency of occurrence is determined\n",
    "#    (4) the word/frequency lists are sorted into order alphabetically\n",
    "#\n",
    "# The \"distance\" between two vectors is the angle between them.\n",
    "# If x = (x1, x2, ..., xn) is the first vector (xi = freq of word i)\n",
    "# and y = (y1, y2, ..., yn) is the second vector,\n",
    "# then the angle between them is defined as:\n",
    "#    d(x,y) = arccos(inner_product(x,y) / (norm(x)*norm(y)))\n",
    "# where:\n",
    "#    inner_product(x,y) = x1*y1 + x2*y2 + ... xn*yn\n",
    "#    norm(x) = sqrt(inner_product(x,x))\n",
    "\n",
    "import math\n",
    "    # math.acos(x) is the arccosine of x.\n",
    "    # math.sqrt(x) is the square root of x.\n",
    "\n",
    "import string\n",
    "    # string.join(words,sep) takes a given list of words,\n",
    "    #    and returns a single string resulting from concatenating them\n",
    "    #    together, separated by the string sep .\n",
    "    # string.lower(word) converts word to lower-case\n",
    "\n",
    "import sys\n",
    "    # sys.exit() allows us to quit (if we can't read a file)\n",
    "\n",
    "# Operation 1: read a text file ##\n",
    "##################################\n",
    "def read_file(filename):\n",
    "    ###    Read the text file with the given filename;\n",
    "    ###     return a list of the lines of text in the file.\n",
    "    try:\n",
    "        fp = open(filename)\n",
    "        L = fp.readlines()\n",
    "    except IOError as excObj:\n",
    "        print(str(excObj))\n",
    "        print(\"Error opening or reading input file: \" + filename)\n",
    "        sys.exit()\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many lines are in assets/t8.shakespeare.txt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "with open('/home/jovyan/work/resources/data/t8.shakespeare.txt', 'r') as file:\n",
    "    # Initialize a counter\n",
    "    line_count = 0\n",
    "    # Iterate through each line in the file\n",
    "    for line in file:\n",
    "        # Increment the counter for each line\n",
    "        line_count += 1\n",
    "\n",
    "print(f'The file has {line_count} lines.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file has 124456 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the function word_frequencies_for_file(...), where is most of the time spent?\n",
    "\n",
    "    > read_file(...)\n",
    "    > get_words_from_line_list(...)\n",
    "    > count_frequency(...)\n",
    "    > insertion_sort(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############################################\n",
    "## compute word frequencies for input file ##\n",
    "#############################################\n",
    "import cProfile\n",
    "\n",
    "def word_frequencies_for_file(filename,verbose=False):\n",
    "\n",
    "    ### Return alphabetically sorted list of (word,frequency) pairs\n",
    "    ### for the given file.\n",
    "\n",
    "    line_list = read_file(filename)\n",
    "    word_list = get_words_from_line_list(line_list)\n",
    "    freq_mapping = count_frequency(word_list)\n",
    "    insertion_sort(freq_mapping)\n",
    "    if verbose:\n",
    "        print(\"File\",filename,\":\", len(line_list),\"lines,\", len(word_list),\"words,\", len(freq_mapping),\"distinct words\")\n",
    "\n",
    "    return freq_mapping\n",
    "\n",
    "def profile_word_frequencies(filename): \n",
    "    word_frequencies_for_file(filename, verbose=True) \n",
    "# Profile the function\n",
    "cProfile.run('profile_word_frequencies(\"/home/jovyan/work/resources/data/t8.shakespeare.txt\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Which of the following commands would result in output that looks similar to:\n",
    "\n",
    "        102 ms Â± 1.98 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
    "\n",
    "    \n",
    "    > %%timeit\n",
    "    > document_similarity('assets/short.t1.txt','assets/short.t2.txt')\n",
    "    > %timeit  document_similarity('assets/short.t1.txt','assets/short.t2.txt')\n",
    "    > %lprun -f document_similarity document_similarity('assets/short.t1.txt','assets/short.t2.txt')\n",
    "    > %lprun -f __main__ document_similarity('assets/short.t1.txt','assets/short.t2.txt')\n",
    "    > Any of the above would generate this output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> %timeit document_similarity('assets/short.t1.txt','assets/short.t2.txt')\n",
    "\n",
    "The %timeit magic command in IPython or Jupyter Notebook is used to time the execution of a single statement. It runs the statement multiple times to get a more accurate measurement of the execution time, providing the mean and standard deviation of the runs.\n",
    "\n",
    "The other commands listed do not produce this specific type of output:\n",
    "\n",
    "    - %%timeit is a cell magic that times the execution of the entire cell, but it doesn't match the exact format of the output shown.\n",
    "\n",
    "    - document_similarity('assets/short.t1.txt','assets/short.t2.txt') simply runs the function without timing it.\n",
    "\n",
    "    - %lprun -f document_similarity document_similarity('assets/short.t1.txt','assets/short.t2.txt') and %lprun -f __main__ document_similarity('assets/short.t1.txt','assets/short.t2.txt') are used with the line_profiler to profile the function line by line, but they do not produce the same output format as %timeit.\n",
    "\n",
    "So, the correct command is %timeit document_similarity('assets/short.t1.txt','assets/short.t2.txt'). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What order is count_frequency()?\n",
    "\n",
    "    > O(1)\n",
    "    - O(n)\n",
    "    > O(n^2)\n",
    "    > O(nlogn)\n",
    "    > O(2^n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count_frequency function has a time complexity of O(n), where n is the number of words in the word_list.\n",
    "\n",
    "Explanation\n",
    "1. Creating the Dictionary:\n",
    "    - Initializing the dictionary frequency_dict is an O(1) operation.\n",
    "2. Iterating Through the Word List:\n",
    "    - The for loop iterates through each word in the word_list, which takes O(n) time, where n is the length of the list.\n",
    "3. Updating the Frequency Count:\n",
    "    - Checking if a word is in the dictionary and updating its count both have an average time complexity of O(1) due to the efficient average-case performance of dictionary operations in Python.\n",
    "4. Converting Dictionary to List:\n",
    "    - Converting the dictionary items to a list using list(frequency_dict.items()) takes O(m) time, where m is the number of unique words. However, since m is generally less than or equal to n, this step is also considered O(n) in the worst case.\n",
    "\n",
    "Combining these steps, the overall time complexity of the count_frequency function is O(n). This means the function scales linearly with the size of the input list. ðŸ“Šâœ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Let's say that you have two files that were the same size and running document_similarity() on them took 18 seconds to complete.  If you then ran document_similarity() on two different files, each of which was twice the size of the original files, and it took 72 seconds to complete, what would that tell you about the order of the overall script?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
